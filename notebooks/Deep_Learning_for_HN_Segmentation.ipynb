{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env requirement\n",
    "# Python (3.7)\n",
    "# pynrrd (0.4) - For loading MICCAI data in .nrrd format\n",
    "# Tqdm - For displaying progress bars\n",
    "# Torchvision (0.8)\n",
    "# MONAI (0.7) - For domain specific models, losses, metrics, etc\n",
    "# pytorch-lightning-1.5.3\n",
    "# pytorch-1.10.0 \n",
    "# torchtext-0.11.0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert 3D Dataset (nrdd to npz)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Dict, List, Tuple, Union\n",
    "import functools\n",
    "from pathlib import Path\n",
    "import nrrd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torchvision.utils import make_grid\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "STRUCTURES: List[str] = [\n",
    "    \"BrainStem\",\n",
    "    \"Chiasm\",\n",
    "    \"Mandible\",\n",
    "    \"OpticNerve_L\",\n",
    "    \"OpticNerve_R\",\n",
    "    \"Parotid_L\",\n",
    "    \"Parotid_R\",\n",
    "    \"Submandibular_L\",\n",
    "    \"Submandibular_R\",\n",
    "]\n",
    "\n",
    "    \n",
    "LANDMARK_COLS: List[str] = [\n",
    "    \"id\",\n",
    "    \"x\",\n",
    "    \"y\",\n",
    "    \"z\",\n",
    "    \"ow\",\n",
    "    \"ox\",\n",
    "    \"oy\",\n",
    "    \"oz\",\n",
    "    \"vis\",\n",
    "    \"sel\",\n",
    "    \"lock\",\n",
    "    \"label\",\n",
    "    \"desc\",\n",
    "    \"associatedNodeID\",\n",
    "]\n",
    "    \n",
    "class AttrDict(dict):\n",
    "    \"\"\"Subclasses dict and define getter-setter. This behaves as both dict and obj\n",
    "    Taken from: https://github.com/facebookresearch/fair-sslime/blob/master/sslime/utils/collections.py\n",
    "    \"\"\"\n",
    "\n",
    "    def __getattr__(self, key):\n",
    "        return self[key]\n",
    "\n",
    "    def __setattr__(self, key, value):\n",
    "        if key in self.__dict__:\n",
    "            self.__dict__[key] = value\n",
    "        else:\n",
    "            self[key] = value\n",
    "\n",
    "    def __delattr__(self, key):\n",
    "        if key in self:\n",
    "            del self[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Volume(object):\n",
    "    def __init__(self, path: str = None, data: Union[np.ndarray, torch.Tensor] = None):\n",
    "        if path is not None:\n",
    "            self._path = path\n",
    "            self._data, self._headers = load_nrrd_as_tensor(path)\n",
    "        else:\n",
    "            assert data is not None, \"Either one of path or data (array) is required\"\n",
    "            self._path = self._headers = None\n",
    "            self._data = self._check_data(data)\n",
    "        self._is_data_modified = False\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Volume(path={self.path})\"\n",
    "\n",
    "    @property\n",
    "    def data(self) -> torch.Tensor:\n",
    "        return self._data\n",
    "\n",
    "    @data.setter\n",
    "    def data(self, arr: Union[np.ndarray, torch.Tensor]) -> None:\n",
    "        arr = self._check_data(arr)\n",
    "        self._data = arr\n",
    "        self._is_data_modified = True\n",
    "\n",
    "    @property\n",
    "    def path(self) -> Union[str, None]:\n",
    "        return self._path\n",
    "\n",
    "    @property\n",
    "    def is_gray(self) -> bool:\n",
    "        return True if self.data.shape[0] == 1 else False\n",
    "\n",
    "    @property\n",
    "    def spacing(self) -> Union[np.ndarray, None]:\n",
    "        if self.headers is not None:\n",
    "            # Reversed the array to align with channel first format\n",
    "            # That is, spacing values in dimension: (z, ..., ...)\n",
    "            return self.headers[\"space directions\"].diagonal()[::-1]\n",
    "        return None\n",
    "\n",
    "    def _check_data(self, data: Union[np.ndarray, torch.Tensor]) -> torch.Tensor:\n",
    "        \"\"\"Only meant to be used internally.\"\"\"\n",
    "        assert len(data.shape) == 4, \"Expected data to be of shape: (C, D, H, W)\"\n",
    "        assert data.shape[0] == 1, \"Expected data to be in channel first format\"\n",
    "        if isinstance(data, np.ndarray):\n",
    "            data = torch.from_numpy(data)\n",
    "\n",
    "        return data\n",
    "\n",
    "    def _crop_data(\n",
    "        self, min_z: int, max_z: int, min_x: int, max_x: int, min_y: int, max_y: int\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Only intended to be used internally. This function performs no checks,\n",
    "        and updates the data according to the given crop information (and holds\n",
    "        no reference to the 'old' data).\n",
    "        All coordinates are expected to be integers.\n",
    "        \"\"\"\n",
    "        self.data = self.data[:, min_z:max_z, min_x:max_x, min_y:max_y]\n",
    "\n",
    "    def as_numpy(self, reverse_dims: bool = False) -> np.ndarray:\n",
    "        arr = self.data.numpy()\n",
    "        if reverse_dims:\n",
    "            arr = np.transpose(arr, (2, 3, 1, 0))\n",
    "        return arr\n",
    "\n",
    "    def as_grid(\n",
    "        self, nrow: int = 4, pad_value: int = 1, reverse_dims: bool = True, **kwargs\n",
    "    ) -> np.ndarray:\n",
    "        grid = make_grid(\n",
    "            self.data.permute(1, 0, 2, 3), nrow=nrow, pad_value=pad_value, **kwargs\n",
    "        )  # Shape: (C, nH, nW)\n",
    "\n",
    "        if self.is_gray:\n",
    "            grid = grid[[0]]  # All channels are the same according to the docs\n",
    "        if reverse_dims:\n",
    "            grid = grid.permute((1, 2, 0))  # Shape: (nH, nW, C)\n",
    "\n",
    "        return grid.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Patient(object):\n",
    "    def __init__(self, patient_dir: str):\n",
    "        self._patient_dir = patient_dir\n",
    "        self.meta_data = self._store_meta_data()\n",
    "\n",
    "        self._image = Volume(self.meta_data[\"image\"])\n",
    "        self._structures = self._load_structures()\n",
    "        if self.meta_data[\"landmarks\"] is not None:\n",
    "            self._landmarks = pd.read_csv(\n",
    "                self.meta_data[\"landmarks\"], comment=\"#\", names=LANDMARK_COLS\n",
    "            )\n",
    "        else:  # No landmarks for test data\n",
    "            self._landmarks = None\n",
    "        self._is_cropped = False\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Patient(patient_dir={self.patient_dir})\"\n",
    "\n",
    "    @property\n",
    "    def image(self) -> Volume:\n",
    "        return self._image\n",
    "\n",
    "    @property\n",
    "    def structures(self) -> AttrDict:\n",
    "        return self._structures\n",
    "\n",
    "    @property\n",
    "    def num_slides(self) -> int:\n",
    "        return self.image.data.shape[1]\n",
    "\n",
    "    @property\n",
    "    def landmarks(self) -> Union[pd.DataFrame, None]:\n",
    "        return self._landmarks\n",
    "\n",
    "    @property\n",
    "    def patient_dir(self) -> str:\n",
    "        return self._patient_dir\n",
    "\n",
    "    def _store_meta_data(self) -> Dict:\n",
    "        meta_data = {\n",
    "            \"image\": None,\n",
    "            \"structures\": {s: None for s in STRUCTURES},\n",
    "            \"landmarks\": None,\n",
    "        }\n",
    "        directory = Path(self.patient_dir)\n",
    "\n",
    "        meta_data[\"image\"] = (directory / \"img.nrrd\").as_posix()\n",
    "        try:\n",
    "            meta_data[\"landmarks\"] = (list(directory.glob(\"*.fcsv\"))[0]).as_posix()\n",
    "        except IndexError:  # No landmarks for test data\n",
    "            meta_data[\"landmarks\"] = None\n",
    "\n",
    "        for structure_path in (directory / \"structures\").iterdir():\n",
    "            meta_data[\"structures\"][structure_path.stem] = structure_path.as_posix()\n",
    "\n",
    "        return meta_data\n",
    "\n",
    "    def _load_structures(self) -> AttrDict:\n",
    "        temp = AttrDict()\n",
    "        for (structure, path) in self.meta_data[\"structures\"].items():\n",
    "            if path is not None:\n",
    "                temp[structure] = Volume(path)\n",
    "            else:\n",
    "                temp[structure] = None\n",
    "\n",
    "        return temp\n",
    "\n",
    "    def crop_data(\n",
    "        self,\n",
    "        boundary_x: Tuple[int, int] = (120, 400),\n",
    "        boundary_y: Tuple[int, int] = (55, 335),\n",
    "        boundary_z: Tuple[float, float] = (0.32, 0.99),\n",
    "    ):\n",
    "        assert np.all(\n",
    "            [isinstance(i, tuple) for i in (boundary_x, boundary_y, boundary_z)]\n",
    "        ), \"Cropping boundary is expected to be a tuple for each axis\"\n",
    "\n",
    "        min_x, max_x = boundary_x\n",
    "        min_y, max_y = boundary_y\n",
    "        min_z, max_z = boundary_z\n",
    "\n",
    "        min_z = np.math.ceil(min_z * self.num_slides)\n",
    "        max_z = np.math.ceil(max_z * self.num_slides)\n",
    "\n",
    "        assert np.all(\n",
    "            [isinstance(i, int) for i in (min_z, max_z, min_x, max_x, min_y, max_y)]\n",
    "        ), (\n",
    "            \"'x' and 'y' coordinates are expected to be integers, and 'z' \"\n",
    "            \"should be float between 0 and 1\"\n",
    "        )\n",
    "        assert min_x < max_x, \"Invalid x-axis boundaries\"\n",
    "        assert min_y < max_y, \"Invalid y-axis boundaries\"\n",
    "        assert min_z < max_z, \"Invalid z-axis boundaries\"\n",
    "\n",
    "        self.image._crop_data(min_z, max_z, min_x, max_x, min_y, max_y)\n",
    "        for structure in STRUCTURES:\n",
    "            if self.structures[structure] is not None:\n",
    "                self.structures[structure]._crop_data(\n",
    "                    min_z, max_z, min_x, max_x, min_y, max_y\n",
    "                )\n",
    "\n",
    "        self._is_cropped = True\n",
    "\n",
    "    def combine_segmentation_masks(self, structure_list: list) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        This is used as a workaround for overlaying multiple segmentation masks\n",
    "        (each corresponding to different region) over a slide. The \"correct\" way\n",
    "        can be quite complicated, and is not worth the time for now.\n",
    "        \"\"\"\n",
    "        assert len(structure_list) > 1, \"A minimum of 2 structures are required\"\n",
    "        structure_arrays = []\n",
    "\n",
    "        for structure in structure_list:\n",
    "            assert structure in STRUCTURES, f\"Invalid structure argument: {structure}\"\n",
    "            structure_volume = self.structures[structure]\n",
    "            if structure_volume is not None:\n",
    "                structure_arrays.append(structure_volume.as_numpy())\n",
    "\n",
    "        combined = functools.reduce(np.logical_or, structure_arrays).astype(\n",
    "            \"uint8\"\n",
    "        )  # Shape: (C, D, H, W)\n",
    "        return combined\n",
    "    \n",
    "    \n",
    "class PatientCollection(object):\n",
    "    def __init__(self, path: str):\n",
    "        self._path = path\n",
    "        self._patient_paths = {\n",
    "            directory.name: directory.as_posix()\n",
    "            for directory in Path(path).glob(\"0522c*\")\n",
    "        }\n",
    "        assert (\n",
    "            len(self._patient_paths) > 0\n",
    "        ), \"No patients found at the specified location: {path}\"\n",
    "\n",
    "    @property\n",
    "    def patient_paths(self) -> Dict:\n",
    "        return self._patient_paths\n",
    "\n",
    "    def apply_function(\n",
    "        self, func: Callable, disable_progress: bool = False, **kwargs\n",
    "    ) -> Dict:\n",
    "        \"\"\"\n",
    "        Applies the callable to each patient, and stores the result in a dictionary.\n",
    "        Any extra keyword arguments will be passed to the callable.\n",
    "        The callable should be of the following form:\n",
    "        def func(patient: Patient, **kwargs):\n",
    "            ...\n",
    "        \"\"\"\n",
    "        iterator = tqdm(self.patient_paths.items(), disable=disable_progress)\n",
    "\n",
    "        collected_results = {\n",
    "            name: func(Patient(path), **kwargs) for (name, path) in iterator\n",
    "        }\n",
    "\n",
    "        return collected_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _patient_to_3d(\n",
    "    patient: Patient, save_location: Path, crop: bool = True,\n",
    ") -> None:\n",
    "    \"\"\"TODO\n",
    "    \"\"\"\n",
    "    temp_patient = patient\n",
    "    if crop:\n",
    "        temp_patient.crop_data()\n",
    "    patient_id = Path(temp_patient.patient_dir).stem\n",
    "    vol = temp_patient.image.as_numpy()\n",
    "\n",
    "    region_slides = []\n",
    "    mask_indicator = np.ones(len(STRUCTURES))\n",
    "    all_zeros = np.zeros_like(\n",
    "        vol[0, :, :, :], dtype=\"uint8\"\n",
    "    )  # Dummy mask. Shape: (D, H, W)\n",
    "    for i, structure in enumerate(STRUCTURES):\n",
    "        region_volume = temp_patient.structures[structure]\n",
    "        if region_volume is not None:\n",
    "            region_slide = region_volume.as_numpy()[0, :, :, :]  # Shape: (D, H, W)\n",
    "        else:\n",
    "            region_slide = all_zeros\n",
    "            mask_indicator[i] = 0\n",
    "        region_slides.append(region_slide)\n",
    "\n",
    "    region_slides = np.stack(\n",
    "        region_slides\n",
    "    )  # Shape: (9, D, H, W) -> 1 mask for each structure\n",
    "\n",
    "    # Ignore the slide if there is no structure (BrainStem, etc) present\n",
    "    # It's useless since there's nothing to train/validate on\n",
    "    if region_slides.sum() > 0:\n",
    "        filename = (save_location / f\"{patient_id}.npz\").as_posix()\n",
    "        np.savez(\n",
    "            filename, image=vol, masks=region_slides, mask_indicator=mask_indicator,\n",
    "        )\n",
    "        \n",
    "def convert_to_3d(\n",
    "    read_dir: str, save_dir: str, split: str = None, crop: bool = True,\n",
    ") -> None:\n",
    "    \"\"\"TODO\n",
    "    \"\"\"\n",
    "    read_location = Path(read_dir)\n",
    "    save_location = Path(save_dir)\n",
    "    if split is not None:\n",
    "        read_location = read_location / split\n",
    "        save_location = save_location / split\n",
    "\n",
    "    save_location.mkdir(parents=True, exist_ok=True)\n",
    "    read_location = read_location.as_posix()\n",
    "\n",
    "    patient_collection = PatientCollection(read_location)\n",
    "    _ = patient_collection.apply_function(\n",
    "        _patient_to_3d, save_location=save_location, crop=crop,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_nrrd_as_tensor(path: str) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Headers are returned without any changes. Should be kept in mind if used with the\n",
    "    tensors, making sure that they both align.\n",
    "    \"\"\"\n",
    "    img, headers = nrrd.read(path)\n",
    "    if img.ndim == 3:  # grayscale, so adding channel=1\n",
    "        img = img[:, :, :, np.newaxis]  # Shape: (H, W, D, C)\n",
    "    tensor = torch.from_numpy(np.transpose(img, (3, 2, 0, 1)))  # Shape: (C, D, H, W)\n",
    "\n",
    "    return (tensor, headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "def is_cluster(cluster_type: str = \"PRINCE\") -> bool:\n",
    "    \"\"\"\n",
    "    Will return True if executed on NYU's HPC Cluster Prince (by default).\n",
    "    \"\"\"\n",
    "    env = os.environ.get(\"CLUSTER\")\n",
    "    return True if env == cluster_type else False\n",
    "\n",
    "\n",
    "def _repository_root() -> Path:\n",
    "    return (Path().resolve().parents[1]).absolute()\n",
    "    # return (Path(__file__).resolve().parents[1]).absolute()\n",
    "\n",
    "\n",
    "def _storage_root() -> Path:\n",
    "    if not is_cluster():\n",
    "        path = _repository_root() / 'My Drive' /\"CT-image-segmentation\" /\"storage\"\n",
    "    else:\n",
    "        beegfs_path = Path(os.environ.get(\"BEEGFS\")).absolute()\n",
    "        path = beegfs_path / \"CT-image-segmentation\" / \"storage\"\n",
    "\n",
    "    return path.absolute()\n",
    "\n",
    "# convert nrrd to npz\n",
    "DEFAULT_DATA_STORAGE = '' #str = _storage_root().as_posix() \n",
    "read_dir = (Path(DEFAULT_DATA_STORAGE) / \"miccai\").as_posix()\n",
    "save_dir = (Path(DEFAULT_DATA_STORAGE) / \"miccai_3d_test\").as_posix()\n",
    "# convert_to_3d(read_dir,save_dir,\"train\")\n",
    "# convert_to_3d(read_dir,save_dir,\"test\")\n",
    "# convert_to_3d(read_dir,save_dir,\"valid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_DATA_STORAGE = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchio.data.subject import Subject\n",
    "from torchio.transforms.preprocessing.intensity.normalization_transform import NormalizationTransform, TypeMaskingMethod\n",
    "\n",
    "_stacked_window_stats = {\"mean\": (0.107, 0.135, 0.085), \"std\": (0.271, 0.267, 0.152)}\n",
    "\n",
    "class Normalization_DIY(NormalizationTransform):\n",
    "    \"\"\"Subtract mean and divide by standard deviation.\n",
    "\n",
    "    Args:\n",
    "        masking_method: See\n",
    "            :class:`~torchio.transforms.preprocessing.intensity.NormalizationTransform`.\n",
    "        **kwargs: See :class:`~torchio.transforms.Transform` for additional\n",
    "            keyword arguments.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            masking_method: TypeMaskingMethod = None,\n",
    "            **kwargs\n",
    "            ):\n",
    "        super().__init__(masking_method=masking_method, **kwargs)\n",
    "        self.args_names = ('masking_method',)\n",
    "\n",
    "    def apply_normalization(\n",
    "            self,\n",
    "            subject: Subject,\n",
    "            image_name: str,\n",
    "            mask: torch.Tensor,\n",
    "            ) -> None:\n",
    "        image = subject[image_name]\n",
    "        standardized = self.znorm(\n",
    "            image.data,\n",
    "            mask,\n",
    "        )\n",
    "        if standardized is None:\n",
    "            message = (\n",
    "                'Standard deviation is 0 for masked values'\n",
    "                f' in image \"{image_name}\" ({image.path})'\n",
    "            )\n",
    "            raise RuntimeError(message)\n",
    "        image.set_data(standardized)\n",
    "\n",
    "    @staticmethod\n",
    "    def znorm(tensor: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n",
    "        tensor = tensor.clone().float()\n",
    "        min_ = -155\n",
    "        max_ = 195\n",
    "        tensor = (tensor - min_) / (max_ - min_ + 1e-8)\n",
    "\n",
    "        return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "_stacked_window_stats = {\"mean\": (0, 0.135, 0.085), \"std\": (1, 0.267, 0.152)}\n",
    "\n",
    "class Normalization_DIY_final(NormalizationTransform):\n",
    "    \"\"\"Subtract mean and divide by standard deviation.\n",
    "\n",
    "    Args:\n",
    "        masking_method: See\n",
    "            :class:`~torchio.transforms.preprocessing.intensity.NormalizationTransform`.\n",
    "        **kwargs: See :class:`~torchio.transforms.Transform` for additional\n",
    "            keyword arguments.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            masking_method: TypeMaskingMethod = None,\n",
    "            **kwargs\n",
    "            ):\n",
    "        super().__init__(masking_method=masking_method, **kwargs)\n",
    "        self.args_names = ('masking_method',)\n",
    "\n",
    "    def apply_normalization(\n",
    "            self,\n",
    "            subject: Subject,\n",
    "            image_name: str,\n",
    "            mask: torch.Tensor,\n",
    "            ) -> None:\n",
    "        image = subject[image_name]\n",
    "        standardized = self.znorm(\n",
    "            image.data,\n",
    "            mask,\n",
    "        )\n",
    "        if standardized is None:\n",
    "            message = (\n",
    "                'Standard deviation is 0 for masked values'\n",
    "                f' in image \"{image_name}\" ({image.path})'\n",
    "            )\n",
    "            raise RuntimeError(message)\n",
    "        image.set_data(standardized)\n",
    "\n",
    "    @staticmethod\n",
    "    def znorm(tensor: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n",
    "        tensor = tensor.clone().float()\n",
    "        mean,std = _stacked_window_stats['mean'][1],_stacked_window_stats['std'][1]\n",
    "        if std == 0:\n",
    "            return None\n",
    "        tensor -= mean\n",
    "        denominator = np.reciprocal(std)\n",
    "        tensor *= denominator\n",
    "        return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchio as tio\n",
    "max_displacement = 2,2,2 \n",
    "num_control_points = 5\n",
    "SIZE = (96,128,224)\n",
    "windowed_degree_0 = {\n",
    "    \"train\": tio.Compose([\n",
    "     tio.ToCanonical(),\n",
    "     tio.CropOrPad(SIZE, mask_name='Segmentation'),\n",
    "     tio.RescaleIntensity(out_min_max=(-155, 195)),\n",
    "     Normalization_DIY(),\n",
    "     tio.OneOf({\n",
    "          tio.RandomElasticDeformation(max_displacement=max_displacement,\\\n",
    "                                       num_control_points=num_control_points):0.3,\n",
    "          tio.RandomAffine(degrees=5,scales=0.1):0.7,\n",
    "    }),\n",
    "     Normalization_DIY_final(),\n",
    "]),\n",
    "    \"test\": tio.Compose([\n",
    "     tio.ToCanonical(),\n",
    "     tio.CropOrPad(SIZE, mask_name='Segmentation'),\n",
    "     tio.RescaleIntensity(out_min_max=(-155, 195)),\n",
    "     Normalization_DIY(),\n",
    "     Normalization_DIY_final(),\n",
    "]),\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Tuple\n",
    "# use torchio not change masks  11.8\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "def get_miccai_3d(split: str = \"train\", transform=None) -> Dataset:\n",
    "    assert split in [\"train\", \"valid\", \"test\"], \"Invalid data split passed\"\n",
    "    path = DEFAULT_DATA_STORAGE + f\"/miccai_3d_test/{split}\"\n",
    "\n",
    "    path = Path(path).absolute()\n",
    "    instance_paths = []\n",
    "    for instance in path.iterdir():\n",
    "            instance_paths.append(instance.as_posix())\n",
    "            \n",
    "      \n",
    "    instance_paths.sort()  # To get same order on Windows and Linux (cluster)\n",
    "    subjects_train = []\n",
    "    for i in range(len(instance_paths)):\n",
    "        instance = np.load(instance_paths[i])\n",
    "        subject_train = tio.Subject(\n",
    "          CT = tio.ScalarImage(tensor=instance['image']),\n",
    "                    Segmentation = tio.LabelMap(tensor=instance['masks']),\n",
    "        )\n",
    "        subjects_train.append(subject_train)\n",
    "    return tio.SubjectsDataset(subjects_train, transform = transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-26 11:39:21.684603: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /gpfs/share/apps/anaconda3/gpu/5.2.0/envs/tz904/lib:/gpfs/share/apps/cuda/9.0/extras/CUPTI/lib64:/gpfs/share/apps/cuda/9.0/lib64:/gpfs/share/apps/cuda/9.0/targets/x86_64-linux/lib\n",
      "2022-05-26 11:39:21.684632: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Optional, Union\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader\n",
    "from multiprocessing import cpu_count\n",
    "import torchio as tio\n",
    "\n",
    "\n",
    "DEGREE = {0: windowed_degree_0}\n",
    "\n",
    "\n",
    "class MiccaiDataModule3D(pl.LightningDataModule):\n",
    "    def __init__(self, batch_size, transform_degree: int = None, **kwargs):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        assert transform_degree in DEGREE.keys(), \"Invalid transform degree passed\"\n",
    "        self.transform = DEGREE[transform_degree]\n",
    "\n",
    "    def setup(self, stage: Optional[str]):\n",
    "        if stage == \"fit\" or stage is None:\n",
    "            self.train_dataset = get_miccai_3d(\n",
    "                split=\"train\", transform=self.transform[\"train\"],\n",
    "            )\n",
    "            self.val_dataset = get_miccai_3d(\n",
    "                split=\"valid\", transform=self.transform[\"test\"], #test\n",
    "            )\n",
    "            print(\"finish set up validation dataset\")\n",
    "\n",
    "        if stage == \"test\" or stage is None:\n",
    "            self.test_dataset = get_miccai_3d(\n",
    "                split=\"test\", transform=self.transform[\"test\"],\n",
    "            )\n",
    "\n",
    "    def train_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            pin_memory=True,\n",
    "            num_workers= cpu_count())#cpu_count())\n",
    "    def val_dataloader(self) -> Union[DataLoader, List[DataLoader]]:\n",
    "        return DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            pin_memory=True,\n",
    "            num_workers= cpu_count()\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self) -> Union[DataLoader, List[DataLoader]]:\n",
    "        return DataLoader(\n",
    "            self.test_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            pin_memory=True,\n",
    "            num_workers= cpu_count()\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning.utilities import rank_zero_only\n",
    "\n",
    "class WandbLoggerPatch(WandbLogger):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    @rank_zero_only\n",
    "    def log_hyperparams(self, params):\n",
    "        params = self._convert_params(params)\n",
    "        params = self._flatten_dict(params)\n",
    "        params = self._sanitize_callable_params(params)\n",
    "        params = self._sanitize_params(params)\n",
    "        self.experiment.config.update(params, allow_val_change=True)\n",
    "\n",
    "def _squash_predictions(preds):\n",
    "    return torch.softmax(preds, dim=1).argmax(dim=1)  # Shape: (N, H, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib created a temporary config/cache directory at /tmp/matplotlib-sw070c0e because the default path (/gpfs/home/tz904/.config/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn.modules.loss import _Loss\n",
    "from monai.utils import LossReduction, MetricReduction, Weight\n",
    "from typing import Callable, Optional, Union\n",
    "\n",
    "class BaseLossWrapper(nn.Module):\n",
    "    \"\"\"TODO\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(BaseLossWrapper, self).__init__()\n",
    "\n",
    "    @property\n",
    "    def loss_fx(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        input, target = self._process(input, target)\n",
    "        return self.loss_fx(input, target)\n",
    "\n",
    "    def _process(self, input, target):\n",
    "        assert target.ndim == 3, \"Expected target of shape: (N, H, W)\"\n",
    "        target = target.unsqueeze(dim=1)  # Shape: (N, 1, H, W)\n",
    "\n",
    "        return (input, target)\n",
    "\n",
    "class MultipleLossWrapper(nn.Module):\n",
    "    def __init__(self, losses, exclude_missing=False):\n",
    "        super(MultipleLossWrapper, self).__init__()\n",
    "        self.exclude_missing = exclude_missing\n",
    "        for name in losses:\n",
    "            assert name in LOSSES.keys()\n",
    "\n",
    "        reduction = \"none\" if self.exclude_missing else \"mean\"\n",
    "        self.losses = nn.ModuleDict(\n",
    "            {name: LOSSES[name](reduction=reduction) for name in losses}\n",
    "        )\n",
    "\n",
    "    def forward(self, input, target, mask_indicator=None, dist_maps=None):\n",
    "        values = {}\n",
    "#         if mask_indicator is not None:\n",
    "#             mask_indicator = mask_indicator.type_as(input)\n",
    "\n",
    "        for (name, fx) in self.losses.items():\n",
    "            if name == \"Boundary\":\n",
    "                assert (\n",
    "                    dist_maps is not None\n",
    "                ), \"Distance maps are required for using boundary loss\"\n",
    "                loss = fx(input, dist_maps)\n",
    "            else:\n",
    "                loss = fx(input, target)  # Either scalar or (N, C)\n",
    "\n",
    "            if self.exclude_missing and (\n",
    "                name not in [\"CrossEntropy\", \"WeightedCrossEntropy\"]\n",
    "            ):\n",
    "                loss = apply_missing_mask(name, loss, mask_indicator)\n",
    "\n",
    "            values[name] = loss\n",
    "\n",
    "        return values\n",
    "\n",
    "class GeneralizedDiceLoss(_Loss):\n",
    "    \"\"\"\n",
    "    Compute the generalised Dice loss defined in:\n",
    "        Sudre, C. et. al. (2017) Generalised Dice overlap as a deep learning\n",
    "        loss function for highly unbalanced segmentations. DLMIA 2017.\n",
    "    Adapted from:\n",
    "        https://github.com/NifTK/NiftyNet/blob/v0.6.0/niftynet/layer/loss_segmentation.py#L279\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        include_background: bool = True,\n",
    "        to_onehot_y: bool = False,\n",
    "        sigmoid: bool = False,\n",
    "        softmax: bool = False,\n",
    "        other_act: Optional[Callable] = None,\n",
    "        w_type: Union[Weight, str] = Weight.SQUARE,\n",
    "        reduction: Union[LossReduction, str] = LossReduction.MEAN,\n",
    "        smooth_nr: float = 1e-5,\n",
    "        smooth_dr: float = 1e-5,\n",
    "        batch: bool = False,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            include_background: If False channel index 0 (background category) is excluded from the calculation.\n",
    "            to_onehot_y: whether to convert `y` into the one-hot format. Defaults to False.\n",
    "            sigmoid: If True, apply a sigmoid function to the prediction.\n",
    "            softmax: If True, apply a softmax function to the prediction.\n",
    "            other_act: if don't want to use `sigmoid` or `softmax`, use other callable function to execute\n",
    "                other activation layers, Defaults to ``None``. for example:\n",
    "                `other_act = torch.tanh`.\n",
    "            squared_pred: use squared versions of targets and predictions in the denominator or not.\n",
    "            w_type: {``\"square\"``, ``\"simple\"``, ``\"uniform\"``}\n",
    "                Type of function to transform ground truth volume to a weight factor. Defaults to ``\"square\"``.\n",
    "            reduction: {``\"none\"``, ``\"mean\"``, ``\"sum\"``}\n",
    "                Specifies the reduction to apply to the output. Defaults to ``\"mean\"``.\n",
    "                - ``\"none\"``: no reduction will be applied.\n",
    "                - ``\"mean\"``: the sum of the output will be divided by the number of elements in the output.\n",
    "                - ``\"sum\"``: the output will be summed.\n",
    "            smooth_nr: a small constant added to the numerator to avoid zero.\n",
    "            smooth_dr: a small constant added to the denominator to avoid nan.\n",
    "            batch: whether to sum the intersection and union areas over the batch dimension before the dividing.\n",
    "                Defaults to False, intersection over union is computed from each item in the batch.\n",
    "        Raises:\n",
    "            TypeError: When ``other_act`` is not an ``Optional[Callable]``.\n",
    "            ValueError: When more than 1 of [``sigmoid=True``, ``softmax=True``, ``other_act is not None``].\n",
    "                Incompatible values.\n",
    "        \"\"\"\n",
    "        super().__init__(reduction=LossReduction(reduction).value)\n",
    "        if other_act is not None and not callable(other_act):\n",
    "            raise TypeError(\n",
    "                f\"other_act must be None or callable but is {type(other_act).__name__}.\"\n",
    "            )\n",
    "        if int(sigmoid) + int(softmax) + int(other_act is not None) > 1:\n",
    "            raise ValueError(\n",
    "                \"Incompatible values: more than 1 of [sigmoid=True, softmax=True, other_act is not None].\"\n",
    "            )\n",
    "        self.include_background = include_background\n",
    "        self.to_onehot_y = to_onehot_y\n",
    "        self.sigmoid = sigmoid\n",
    "        self.softmax = softmax\n",
    "        self.other_act = other_act\n",
    "\n",
    "        w_type = Weight(w_type)\n",
    "        self.w_func: Callable = torch.ones_like\n",
    "        if w_type == Weight.SIMPLE:\n",
    "            self.w_func = torch.reciprocal\n",
    "        elif w_type == Weight.SQUARE:\n",
    "            self.w_func = lambda x: torch.reciprocal(x * x)\n",
    "\n",
    "        self.smooth_nr = float(smooth_nr)\n",
    "        self.smooth_dr = float(smooth_dr)\n",
    "        self.batch = batch\n",
    "\n",
    "    def forward(self, input: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input: the shape should be BNH[WD].\n",
    "            target: the shape should be BNH[WD].\n",
    "        Raises:\n",
    "            ValueError: When ``self.reduction`` is not one of [\"mean\", \"sum\", \"none\"].\n",
    "        \"\"\"\n",
    "        if self.sigmoid:\n",
    "            input = torch.sigmoid(input)\n",
    "        n_pred_ch = input.shape[1]\n",
    "        if self.softmax:\n",
    "            if n_pred_ch == 1:\n",
    "                warnings.warn(\"single channel prediction, `softmax=True` ignored.\")\n",
    "            else:\n",
    "                input = torch.softmax(input, 1)\n",
    "\n",
    "        if self.other_act is not None:\n",
    "            input = self.other_act(input)\n",
    "\n",
    "        if self.to_onehot_y:\n",
    "            if n_pred_ch == 1:\n",
    "                warnings.warn(\"single channel prediction, `to_onehot_y=True` ignored.\")\n",
    "            else:\n",
    "                target = one_hot(target, num_classes=n_pred_ch)\n",
    "\n",
    "        if not self.include_background:\n",
    "            if n_pred_ch == 1:\n",
    "                warnings.warn(\n",
    "                    \"single channel prediction, `include_background=False` ignored.\"\n",
    "                )\n",
    "            else:\n",
    "                # if skipping background, removing first channel\n",
    "                target = target[:, 1:]\n",
    "                input = input[:, 1:]\n",
    "\n",
    "        assert (\n",
    "            target.shape == input.shape\n",
    "        ), f\"ground truth has differing shape ({target.shape}) from input ({input.shape})\"\n",
    "\n",
    "        # reducing only spatial dimensions (not batch nor channels)\n",
    "        reduce_axis = list(range(2, len(input.shape)))\n",
    "        if self.batch:\n",
    "            reduce_axis = [0] + reduce_axis\n",
    "        intersection = torch.sum(target * input, reduce_axis)\n",
    "\n",
    "        ground_o = torch.sum(target, reduce_axis)\n",
    "        pred_o = torch.sum(input, reduce_axis)\n",
    "\n",
    "        denominator = ground_o + pred_o\n",
    "\n",
    "        w = self.w_func(ground_o.float())\n",
    "        for b in w:\n",
    "            infs = torch.isinf(b)\n",
    "            b[infs] = 0.0\n",
    "            b[infs] = torch.max(b)\n",
    "\n",
    "        f: torch.Tensor = 1.0 - (2.0 * (intersection * w) + self.smooth_nr) / (\n",
    "            (denominator * w) + self.smooth_dr\n",
    "        )\n",
    "\n",
    "        if self.reduction == LossReduction.MEAN.value:\n",
    "            f = torch.mean(f)  # the batch and channel average\n",
    "        elif self.reduction == LossReduction.SUM.value:\n",
    "            f = torch.sum(f)  # sum over the batch and channel dims\n",
    "        elif self.reduction == LossReduction.NONE.value:\n",
    "            pass  # returns [N, n_classes] losses\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f'Unsupported reduction: {self.reduction}, available options are [\"mean\", \"sum\", \"none\"].'\n",
    "            )\n",
    "\n",
    "        return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from monai.losses.dice import DiceLoss\n",
    "from monai.losses.focal_loss import FocalLoss\n",
    "from monai.transforms import AsDiscrete\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "WEIGHT = {\n",
    "    \"Background\": 1e-10,\n",
    "    \"BrainStem\": 0.007,\n",
    "    \"Chiasm\": 0.3296,\n",
    "    \"Mandible\": 0.0046,\n",
    "    \"OpticNerve_L\": 0.2619,\n",
    "    \"OpticNerve_R\": 0.3035,\n",
    "    \"Parotid_L\": 0.0068,\n",
    "    \"Parotid_R\": 0.0065,\n",
    "    \"Submandibular_L\": 0.0374,\n",
    "    \"Submandibular_R\": 0.0426,\n",
    "}  # Inverse pixel-frequency (Background is given no weight)\n",
    "\n",
    "\n",
    "class BaseLossWrapper3D(BaseLossWrapper):\n",
    "    \"\"\"TODO\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(BaseLossWrapper3D, self).__init__()\n",
    "\n",
    "    def _process(self, input, target):\n",
    "        # assert target.ndim == 3, \"Expected target of shape: (N, H, W)\"\n",
    "        target = target.unsqueeze(dim=1)  # Shape: (N, 1, H, W)\n",
    "\n",
    "        return (input, target)\n",
    "\n",
    "\n",
    "class CrossEntropyWrapper3D(BaseLossWrapper3D):\n",
    "    \"\"\"TODO\"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(CrossEntropyWrapper3D, self).__init__()\n",
    "\n",
    "    @property\n",
    "    def loss_fx(self):\n",
    "        return F.cross_entropy\n",
    "\n",
    "    def _process(self, input, target):\n",
    "        return (input, target)\n",
    "\n",
    "\n",
    "class WeightedCrossEntropyWrapper3D(CrossEntropyWrapper3D):\n",
    "    \"\"\"TODO\"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(WeightedCrossEntropyWrapper3D, self).__init__()\n",
    "        self.weight = torch.as_tensor(list(WEIGHT.values()))\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        input, target = self._process(input, target)\n",
    "        return self.loss_fx(input, target, weight=self.weight.type_as(input))\n",
    "\n",
    "\n",
    "class DiceLossWrapper3D(BaseLossWrapper3D):\n",
    "    \"\"\"TODO\"\"\"\n",
    "\n",
    "    def __init__(self, reduction=\"mean\"):\n",
    "        super(DiceLossWrapper3D, self).__init__()\n",
    "        self.reduction = reduction\n",
    "        print(\"reduction inside dicelosswrapper3d\",reduction)\n",
    "\n",
    "    @property\n",
    "    def loss_fx(self):\n",
    "        return DiceLoss(\n",
    "            include_background=False,\n",
    "            to_onehot_y=True,\n",
    "            softmax=True,\n",
    "            reduction=self.reduction,\n",
    "        )\n",
    "\n",
    "\n",
    "class GeneralizedDiceLossWrapper3D(BaseLossWrapper3D):\n",
    "    \"\"\"TODO\"\"\"\n",
    "\n",
    "    def __init__(self, reduction=\"mean\"): \n",
    "        super(GeneralizedDiceLossWrapper3D, self).__init__()\n",
    "        self.reduction = reduction\n",
    "\n",
    "    @property\n",
    "    def loss_fx(self):\n",
    "        return GeneralizedDiceLoss(\n",
    "            include_background=False,\n",
    "            to_onehot_y=True,\n",
    "            softmax=True,\n",
    "            reduction=self.reduction,\n",
    "        )\n",
    "\n",
    "\n",
    "class FocalLossWrapper3D(BaseLossWrapper3D):\n",
    "    \"\"\"TODO\"\"\"\n",
    "\n",
    "    def __init__(self, reduction=\"mean\"):\n",
    "        super(FocalLossWrapper3D, self).__init__()\n",
    "        self.reduction = reduction\n",
    "        self.n_classes = len(STRUCTURES) + 1  # Additional background\n",
    "\n",
    "    @property\n",
    "    def loss_fx(self):\n",
    "        return FocalLoss(reduction=self.reduction)\n",
    "\n",
    "    def _process(self, input, target):\n",
    "        target = target.unsqueeze(dim=0)  #1 # Shape: (N, 1, H, W) \n",
    "        \n",
    "        expand = AsDiscrete(to_onehot=True, n_classes=self.n_classes)\n",
    "        target = expand(target)\n",
    "        target = target.reshape((target.shape[1],target.shape[0],target.shape[2],target.shape[3],target.shape[4]))\n",
    "        return (input,target)\n",
    "\n",
    "\n",
    "LOSSES = {\n",
    "    \"CrossEntropy\": CrossEntropyWrapper3D,\n",
    "    \"WeightedCrossEntropy\": WeightedCrossEntropyWrapper3D,\n",
    "    \"Focal\": FocalLossWrapper3D,\n",
    "    \"Dice\": DiceLossWrapper3D,\n",
    "    \"GeneralizedDice\": GeneralizedDiceLossWrapper3D,\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultipleLossWrapper3D(MultipleLossWrapper):\n",
    "    def __init__(self, losses, exclude_missing=False):\n",
    "        super(MultipleLossWrapper3D, self).__init__(losses, exclude_missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.transforms import AsDiscrete\n",
    "from typing import Callable, Optional, Union\n",
    "import warnings\n",
    "from functools import partial\n",
    "from monai.networks import one_hot\n",
    "from monai.utils import LossReduction, MetricReduction, Weight\n",
    "import torch\n",
    "from torch.nn.modules.loss import _Loss\n",
    "from monai.metrics import HausdorffDistanceMetric,compute_hausdorff_distance\n",
    "\n",
    "def compute_meandice(\n",
    "    y_pred: torch.Tensor, y: torch.Tensor, include_background: bool = True,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Computes Dice score metric from full size Tensor and collects average.\n",
    "    Args:\n",
    "        y_pred: input data to compute, typical segmentation model output.\n",
    "            It must be one-hot format and first dim is batch, example shape: [16, 3, 32, 32]. The values\n",
    "            should be binarized.\n",
    "        y: ground truth to compute mean dice metric. It must be one-hot format and first dim is batch.\n",
    "            The values should be binarized.\n",
    "        include_background: whether to skip Dice computation on the first channel of\n",
    "            the predicted output. Defaults to True.\n",
    "    Returns:\n",
    "        Dice scores per batch and per class, (shape [batch_size, n_classes]).\n",
    "    Raises:\n",
    "        ValueError: when `y_pred` and `y` have different shapes.\n",
    "    \"\"\"\n",
    "\n",
    "    if not include_background:\n",
    "        y_pred, y = ignore_background(y_pred=y_pred, y=y,)\n",
    "\n",
    "    y = y.float()\n",
    "    y_pred = y_pred.float()\n",
    "\n",
    "    if y.shape != y_pred.shape:\n",
    "        raise ValueError(\"y_pred and y should have same shapes.\")\n",
    "\n",
    "    # reducing only spatial dimensions (not batch nor channels)\n",
    "    n_len = len(y_pred.shape)\n",
    "    reduce_axis = list(range(2, n_len))\n",
    "    intersection = torch.sum(y * y_pred, dim=reduce_axis)\n",
    "\n",
    "    y_o = torch.sum(y, reduce_axis)\n",
    "    y_pred_o = torch.sum(y_pred, dim=reduce_axis)\n",
    "    denominator = y_o + y_pred_o\n",
    "\n",
    "    f = torch.where(\n",
    "        y_o > 0,\n",
    "        (2.0 * intersection) / denominator,\n",
    "        torch.tensor(float(\"nan\"), device=y_o.device),\n",
    "    )\n",
    "    return f  # returns array of Dice with shape: [batch, n_classes]\n",
    "\n",
    "\n",
    "def do_metric_reduction(\n",
    "    f: torch.Tensor, reduction: Union[MetricReduction, str] = MetricReduction.MEAN,\n",
    "):\n",
    "    \"\"\"\n",
    "    This function is to do the metric reduction for calculated metrics of each example's each class.\n",
    "    Args:\n",
    "        f: a tensor that contains the calculated metric scores per batch and\n",
    "            per class. The first two dims should be batch and class.\n",
    "        reduction: {``\"none\"``, ``\"mean\"``, ``\"sum\"``, ``\"mean_batch\"``, ``\"sum_batch\"``,\n",
    "        ``\"mean_channel\"``, ``\"sum_channel\"``}\n",
    "        Define the mode to reduce computation result of 1 batch data. Defaults to ``\"mean\"``.\n",
    "    Raises:\n",
    "        ValueError: When ``reduction`` is not one of\n",
    "            [\"mean\", \"sum\", \"mean_batch\", \"sum_batch\", \"mean_channel\", \"sum_channel\" \"none\"].\n",
    "    \"\"\"\n",
    "\n",
    "    # some elements might be Nan (if ground truth y was missing (zeros))\n",
    "    # we need to account for it\n",
    "    nans = torch.isnan(f)\n",
    "    not_nans = (~nans).float()\n",
    "    f[nans] = 0\n",
    "\n",
    "    t_zero = torch.zeros(1, device=f.device, dtype=torch.float)\n",
    "    reduction = MetricReduction(reduction)\n",
    "\n",
    "    if reduction == MetricReduction.MEAN:\n",
    "        # 2 steps, first, mean by channel (accounting for nans), then by batch\n",
    "        not_nans = not_nans.sum(dim=1)\n",
    "        f = torch.where(\n",
    "            not_nans > 0, f.sum(dim=1) / not_nans, t_zero\n",
    "        )  # channel average\n",
    "\n",
    "        not_nans = (not_nans > 0).float().sum(dim=0)\n",
    "        f = torch.where(not_nans > 0, f.sum(dim=0) / not_nans, t_zero)  # batch average\n",
    "\n",
    "    elif reduction == MetricReduction.SUM:\n",
    "        not_nans = not_nans.sum(dim=[0, 1])\n",
    "        f = torch.sum(f, dim=[0, 1])  # sum over the batch and channel dims\n",
    "    elif reduction == MetricReduction.MEAN_BATCH:\n",
    "        not_nans = not_nans.sum(dim=0)\n",
    "        f = torch.where(not_nans > 0, f.sum(dim=0) / not_nans, t_zero)  # batch average\n",
    "    elif reduction == MetricReduction.SUM_BATCH:\n",
    "        not_nans = not_nans.sum(dim=0)\n",
    "        f = f.sum(dim=0)  # the batch sum\n",
    "    elif reduction == MetricReduction.MEAN_CHANNEL:\n",
    "        not_nans = not_nans.sum(dim=1)\n",
    "        f = torch.where(\n",
    "            not_nans > 0, f.sum(dim=1) / not_nans, t_zero\n",
    "        )  # channel average\n",
    "    elif reduction == MetricReduction.SUM_CHANNEL:\n",
    "        not_nans = not_nans.sum(dim=1)\n",
    "        f = f.sum(dim=1)  # the channel sum\n",
    "    elif reduction == MetricReduction.NONE:\n",
    "        pass\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"Unsupported reduction: {reduction}, available options are \"\n",
    "            '[\"mean\", \"sum\", \"mean_batch\", \"sum_batch\", \"mean_channel\", \"sum_channel\" \"none\"].'\n",
    "        )\n",
    "    return f, not_nans\n",
    "\n",
    "class DiceMetricWrapper(object):\n",
    "    \"\"\"TODO\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.metric_fx = partial(compute_meandice, include_background=False)\n",
    "        self.n_classes = len(STRUCTURES) + 1  # Additional background\n",
    "\n",
    "    def __call__(self, input, target):\n",
    "        input, target = self._process(input, target)\n",
    "        input = input.reshape((input.shape[1],input.shape[0],input.shape[2],input.shape[3],input.shape[4]))\n",
    "        target = target.reshape((target.shape[1],target.shape[0],target.shape[2],target.shape[3],target.shape[4]))\n",
    "\n",
    "        score = self.metric_fx(input, target)  # Shape: (N, C)\n",
    "        \n",
    "        dice_per_class = do_metric_reduction(score, \"mean_batch\")[0]\n",
    "        dice_mean = dice_per_class.mean()\n",
    "        return dice_mean, dice_per_class\n",
    "\n",
    "    def _process(self, input, target):\n",
    "        assert input.ndim == 3, \"Expected input of shape: (N, H, W)\"\n",
    "        assert target.ndim == 3, \"Expected target of shape: (N, H, W)\"\n",
    "\n",
    "        input = input.unsqueeze(dim=0)  # Shape: (N, 1, H, W) #1\n",
    "        target = target.unsqueeze(dim=0)  # Shape: (N, 1, H, W) #1\n",
    "\n",
    "        expand = AsDiscrete(to_onehot=True, n_classes=self.n_classes)\n",
    "        return expand(input), expand(target)  # Shape: (N, C, H, W) - binarized\n",
    "\n",
    "class DiceMetricWrapper3D(DiceMetricWrapper):\n",
    "    \"\"\"TODO\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(DiceMetricWrapper3D, self).__init__()\n",
    "\n",
    "    def _process(self, input, target):\n",
    "        # remove for now, consider making maybe 3D loss function\n",
    "        # assert input.ndim == 3, \"Expected input of shape: (N, H, W)\"\n",
    "        # assert target.ndim == 3, \"Expected target of shape: (N, H, W)\"\n",
    "\n",
    "        input = input.unsqueeze(dim=0)  # Shape: (N, 1, H, W)\n",
    "        target = target.unsqueeze(dim=0)  # Shape: (N, 1, H, W)\n",
    "\n",
    "        expand = AsDiscrete(to_onehot=True, n_classes=self.n_classes)\n",
    "        return expand(input), expand(target)  # Shape: (N, C, H, W) - binarized\n",
    "    \n",
    "\n",
    "from monai.metrics.utils import do_metric_reduction as do_metric_reduction_hdm\n",
    "\n",
    "class HausdorffDistanceMetricWrapper(object):\n",
    "    \"\"\"TODO\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.metric_fx = partial(compute_hausdorff_distance, include_background=False)\n",
    "        self.n_classes = len(STRUCTURES) + 1  # Additional background\n",
    "\n",
    "    def __call__(self, input, target):\n",
    "        input, target = self._process(input, target)\n",
    "        input = input.reshape((input.shape[1],input.shape[0],input.shape[2],input.shape[3],input.shape[4]))\n",
    "        target = target.reshape((target.shape[1],target.shape[0],target.shape[2],target.shape[3],target.shape[4]))\n",
    "\n",
    "        score = self.metric_fx(input, target,percentile=95)  # Shape: (N, C)\n",
    "        \n",
    "#         dice_per_class = do_metric_reduction(score, \"mean_batch\")[0]\n",
    "#         dice_mean = dice_per_class.mean()\n",
    "\n",
    "        dice_per_class = do_metric_reduction_hdm(score,'mean_batch')[0]\n",
    "        dice_mean = dice_per_class.mean()\n",
    "        return dice_mean, dice_per_class\n",
    "\n",
    "    def _process(self, input, target):\n",
    "        assert input.ndim == 3, \"Expected input of shape: (N, H, W)\"\n",
    "        assert target.ndim == 3, \"Expected target of shape: (N, H, W)\"\n",
    "\n",
    "        input = input.unsqueeze(dim=0)  # Shape: (N, 1, H, W) #1\n",
    "        target = target.unsqueeze(dim=0)  # Shape: (N, 1, H, W) #1\n",
    "\n",
    "        expand = AsDiscrete(to_onehot=True, n_classes=self.n_classes)\n",
    "        return expand(input), expand(target)  # Shape: (N, C, H, W) - binarized\n",
    "    \n",
    "    \n",
    "class HausdorffDistanceMetricWrapper3D(HausdorffDistanceMetricWrapper):\n",
    "    \"\"\"TODO\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(HausdorffDistanceMetricWrapper3D, self).__init__()\n",
    "\n",
    "    def _process(self, input, target):\n",
    "        # remove for now, consider making maybe 3D loss function\n",
    "        # assert input.ndim == 3, \"Expected input of shape: (N, H, W)\"\n",
    "        # assert target.ndim == 3, \"Expected target of shape: (N, H, W)\"\n",
    "\n",
    "        input = input.unsqueeze(dim=0)  # Shape: (N, 1, H, W)\n",
    "        target = target.unsqueeze(dim=0)  # Shape: (N, 1, H, W)\n",
    "\n",
    "        expand = AsDiscrete(to_onehot=True, n_classes=self.n_classes)\n",
    "        return expand(input), expand(target)  # Shape: (N, C, H, W) - binarized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.networks.nets import UNet,UNETR\n",
    "from argparse import ArgumentParser\n",
    "from typing import List\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import seed_everything, Trainer\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "to_do_lst = [[],[]]\n",
    "\n",
    "def _squash_masks_3D(masks, n_classes, device):\n",
    "    _temp = torch.arange(1, n_classes, device=device)\n",
    "    masks = (masks * _temp[None, :, None, None, None]).max(dim=1).values\n",
    "    return masks\n",
    "\n",
    "class BaseUNet3D(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        filters: List = [16, 32, 64, 128, 256], #[16, 32, 64, 128, 256]\n",
    "        use_res_units: bool = False,\n",
    "        downsample: bool = False,\n",
    "        lr: float = 1e-3,\n",
    "        loss_fx: list = [\"CrossEntropy\"],\n",
    "        exclude_missing: bool = False,\n",
    "        model: str ='3dunet',\n",
    "        **kwargs,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        assert isinstance(loss_fx, list), \"This module expects a list of loss functions\"\n",
    "        loss_fx.sort()  # To have consistent order of loss functions\n",
    "        self.model = model\n",
    "        self.save_hyperparameters(\n",
    "            \"batch_size\",\n",
    "            \"transform_degree\",\n",
    "            \"filters\",\n",
    "            \"use_res_units\",\n",
    "            \"downsample\",\n",
    "            \"lr\",\n",
    "            \"loss_fx\",\n",
    "            \"exclude_missing\",\n",
    "            \"model\",\n",
    "        )\n",
    "\n",
    "        self.unet = self._construct_model()\n",
    "        self.loss_func = MultipleLossWrapper3D(\n",
    "            losses=loss_fx, exclude_missing=exclude_missing\n",
    "        )\n",
    "        self.dice_score = DiceMetricWrapper3D()\n",
    "        self.hausdorff_distance  = HausdorffDistanceMetricWrapper3D()\n",
    "        self.weight_focal = 0.1\n",
    "        self.weight_weightedcrossentropy = 1\n",
    "\n",
    "    @property\n",
    "    def _n_classes(self):\n",
    "        return len(STRUCTURES) + 1  # Additional background\n",
    "\n",
    "    def _construct_model(self):\n",
    "        # in_channels = (\n",
    "        #    1 if self.hparams.downsample else 3\n",
    "        # )  # assuming transform_degree in [1, 2, 3, 4]\n",
    "        strides = [2, 2, 2, 2]  # Default for 5-layer UNet\n",
    "        in_channels = 1  \n",
    "        if self.model == '3dunet':#[3dunet,transuneter]\n",
    "            return UNet(\n",
    "            dimensions=3,\n",
    "            in_channels=in_channels,\n",
    "            out_channels=self._n_classes,\n",
    "            channels=self.hparams.filters,\n",
    "            strides=strides,\n",
    "            num_res_units=3,\n",
    "            )\n",
    "        elif  self.model == 'transuneter':\n",
    "            return UNETR(in_channels=in_channels, \n",
    "                    out_channels=self._n_classes, \n",
    "                    img_size=SIZE, #(96,128,224)\n",
    "                    feature_size= 32, \n",
    "                    norm_name='batch')\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.unet(x)\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # print(\"start training\")\n",
    "        _, _, _, _, loss = self._shared_step(batch, prefix = 'train')\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        self._shared_step(batch, prefix='val')\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        self._shared_step(batch, prefix='test')\n",
    "\n",
    "    def _shared_step(self, batch, prefix: str):\n",
    "        # Image : Bx1xHxWxD (4x1x96x128x224 usually) #Masks : Bx9xHxWxD (1x9x96x128x224  usually)\n",
    "        (images, masks) = batch['CT'][tio.DATA],batch['Segmentation'][tio.DATA]\n",
    "        mask_indicator = np.ones(len(STRUCTURES))\n",
    " \n",
    "        masks = _squash_masks_3D(\n",
    "            masks, self._n_classes, self.device\n",
    "        )  # Masks: BxHxWxD (4x96x128x224  usually) has number in [0-9] indicating one of 10 classes\n",
    "\n",
    "        prediction = self.forward(\n",
    "            images\n",
    "        )  # Prediction: Bx10xHxWxD (4x10x96x128x224 usually)\n",
    "        masks = masks.type(torch.cuda.LongTensor)\n",
    "        loss_dict = self.loss_func(\n",
    "            input=prediction, target=masks, mask_indicator=mask_indicator\n",
    "        )\n",
    "        total_loss = 0\n",
    "        for name, loss_value in loss_dict.items():\n",
    "            if name == \"GeneralizedDice\":\n",
    "                total_loss += loss_value\n",
    "            elif name ==\"Focal\":\n",
    "                total_loss += self.weight_focal * loss_value\n",
    "            elif name == \"WeightedCrossEntropy\":\n",
    "                total_loss += self.weight_weightedcrossentropy * loss_value\n",
    "            self.log(\n",
    "                f\"{name} Loss ({prefix})\", loss_value, on_step=False, on_epoch=True,\n",
    "            )\n",
    "\n",
    "        self._log_dice_scores(prediction, masks, mask_indicator, prefix)\n",
    "        return images, masks, mask_indicator, prediction, total_loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return optim.Adam(self.parameters(), lr=self.hparams.lr)\n",
    "\n",
    "    def _log_dice_scores(self, prediction, masks, mask_indicator, prefix):\n",
    "        pred = prediction.clone()\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            if self.hparams.exclude_missing:\n",
    "#             No indicator for background\n",
    "              pred[:, 1:, :, :] = pred[:, 1:, :, :] * mask_indicator[:, :, None, None]\n",
    "            \n",
    "            \n",
    "            pred = _squash_predictions(pred)  # Shape: (N, H, W)\n",
    "            hausdorff_distance_mean, hausdorff_distance_per_class = self.hausdorff_distance(pred,masks)\n",
    "            dice_mean, dice_per_class = self.dice_score(pred, masks)\n",
    "            for structure, score in zip(STRUCTURES, dice_per_class):\n",
    "                self.log(\n",
    "                    f\"{structure} Dice ({prefix})\", score, on_step=False, on_epoch=True,\n",
    "                )\n",
    "            self.log(\n",
    "                f\"Mean Dice Score ({prefix})\", dice_mean, on_step=False, on_epoch=True,\n",
    "            )\n",
    "            \n",
    "            for structure, score in zip(STRUCTURES, hausdorff_distance_per_class):\n",
    "                self.log(\n",
    "                    f\"{structure} Hausdorff distance ({prefix})\", score, on_step=False, on_epoch=True,\n",
    "                )\n",
    "            self.log(\n",
    "                f\"Mean Hausdorff distance ({prefix})\", hausdorff_distance_mean, on_step=False, on_epoch=True,\n",
    "            )\n",
    "\n",
    "            \n",
    "        if prefix == \"val\":\n",
    "          if dice_mean > 0.2:\n",
    "              self.weight_weightedcrossentropy = 0.1 \n",
    "              self.weight_focal = 0.5 \n",
    "          elif dice_mean > 0.4:\n",
    "              self.weight_weightedcrossentropy = 0.01 \n",
    "              self.weight_focal = 1\n",
    "              self.hparams.lr = self.hparams.lr/10\n",
    "          elif dice_mean > 0.5:\n",
    "              self.hparams.lr = self.hparams.lr/2\n",
    "        self.train()\n",
    "\n",
    "    @staticmethod\n",
    "    def add_model_specific_args(parent_parser):\n",
    "        \"\"\"The parameters specific to the model/data processing.\"\"\"\n",
    "        parser = ArgumentParser(parents=[parent_parser], add_help=False)\n",
    "        parser.add_argument(\n",
    "            \"--batch_size\", type=int, default=1, help=\"Batch size\",\n",
    "        )\n",
    "        parser.add_argument(\n",
    "            \"--transform_degree\",\n",
    "            type=int,\n",
    "            default=0,\n",
    "            help=\"The degree of transforms/data augmentation to be applied\",\n",
    "        )\n",
    "        parser.add_argument(\n",
    "            \"--filters\",\n",
    "            nargs=5,\n",
    "            type=int,\n",
    "            default=[64, 128, 256, 512, 1024],\n",
    "            help=\"A sqeuence of number of filters for the downsampling path in UNet\",\n",
    "        )\n",
    "        parser.add_argument(\n",
    "            \"--use_res_units\",\n",
    "            action=\"store_true\",\n",
    "            default=False,\n",
    "            help=\"For using residual units in UNet\",\n",
    "        )\n",
    "        parser.add_argument(\n",
    "            \"--downsample\",\n",
    "            action=\"store_true\",\n",
    "            default=False,\n",
    "            help=\"For using a 1x1 convolution to downsample the input before UNet\",\n",
    "        )\n",
    "        parser.add_argument(\n",
    "            \"--lr\", type=float, default=1e-3, help=\"Learning rate\",\n",
    "        )\n",
    "        parser.add_argument(\n",
    "            \"--loss_fx\",\n",
    "            nargs=\"+\",\n",
    "            type=str,\n",
    "            default=\"CrossEntropy\",\n",
    "            help=\"Loss function\",\n",
    "        )\n",
    "        parser.add_argument(\n",
    "            \"--exclude_missing\",\n",
    "            action=\"store_true\",\n",
    "            default=False,\n",
    "            help=\"Exclude missing annotations from loss computation as described in AnatomyNet\",\n",
    "        )\n",
    "        return parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ignore_background(\n",
    "    y_pred: torch.Tensor, y: torch.Tensor,\n",
    "):\n",
    "    \"\"\"\n",
    "    This function is used to remove background (the first channel) for `y_pred` and `y`.\n",
    "    Args:\n",
    "        y_pred: predictions. As for classification tasks,\n",
    "            `y_pred` should has the shape [BN] where N is larger than 1. As for segmentation tasks,\n",
    "            the shape should be [BNHW] or [BNHWD].\n",
    "        y: ground truth, the first dim is batch.\n",
    "    \"\"\"\n",
    "    y = y[:, 1:] if y.shape[1] > 1 else y\n",
    "    y_pred = y_pred[:, 1:] if y_pred.shape[1] > 1 else y_pred\n",
    "    return y_pred, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wandb import Image\n",
    "from pytorch_lightning.callbacks import Callback\n",
    "\n",
    "class ExamplesLoggingCallback(Callback):\n",
    "    \"\"\"Callback to upload sample predictions to W&B.\"\"\"\n",
    "\n",
    "    def __init__(self, log_every_n_epochs=25, seed=None) -> None:\n",
    "        super().__init__()\n",
    "        self.log_every_n_epochs = log_every_n_epochs\n",
    "        self.rng = np.random.default_rng(seed=seed)\n",
    "\n",
    "    def on_fit_start(self, trainer, pl_module):\n",
    "        self.sample_indices = self.rng.choice(\n",
    "            np.arange(pl_module.hparams.batch_size, dtype=int),\n",
    "            size=min(pl_module.hparams.batch_size, 50),\n",
    "            replace=False,\n",
    "        )\n",
    "        self.num_val_batches = len(trainer.datamodule.val_dataloader())\n",
    "\n",
    "    def on_validation_batch_end(\n",
    "        self, trainer, pl_module, outputs, batch, batch_idx, dataloader_idx\n",
    "    ):\n",
    "        if (trainer.current_epoch % self.log_every_n_epochs == 0) and (\n",
    "            (batch_idx + 1) != self.num_val_batches\n",
    "        ):\n",
    "            with torch.no_grad():\n",
    "                sample_images, sample_masks, sample_preds = self._make_predictions(\n",
    "                    batch, pl_module\n",
    "                )\n",
    "                self._log_images(\n",
    "                    sample_images,\n",
    "                    sample_masks,\n",
    "                    sample_preds,\n",
    "                    pl_module,\n",
    "                    \"val_sample_predictions\",\n",
    "                )\n",
    "\n",
    "    def on_test_batch_end(\n",
    "        self, trainer, pl_module, outputs, batch, batch_idx, dataloader_idx\n",
    "    ):\n",
    "        num_test_batches = len(trainer.datamodule.test_dataloader())\n",
    "        if (batch_idx + 1) != num_test_batches:\n",
    "            with torch.no_grad():\n",
    "                sample_images, sample_masks, sample_preds = self._make_predictions(\n",
    "                    batch, pl_module\n",
    "                )\n",
    "                self._log_images(\n",
    "                    sample_images,\n",
    "                    sample_masks,\n",
    "                    sample_preds,\n",
    "                    pl_module,\n",
    "                    \"test_sample_predictions\",\n",
    "                )\n",
    "\n",
    "    def _make_predictions(self, batch, pl_module):\n",
    "        images, masks, mask_indicator, *others = batch\n",
    "        images = images.to(pl_module.device)\n",
    "        masks = masks.to(pl_module.device)\n",
    "        mask_indicator = mask_indicator.to(pl_module.device).type_as(images)\n",
    "\n",
    "        masks = _squash_masks(masks, pl_module._n_classes, pl_module.device)\n",
    "\n",
    "        sample_images = images[self.sample_indices]\n",
    "        sample_masks = masks[self.sample_indices]\n",
    "        sample_mask_indicator = mask_indicator[self.sample_indices]\n",
    "        sample_preds = pl_module.forward(sample_images)\n",
    "        if pl_module.hparams.exclude_missing:\n",
    "            # No indicator for background\n",
    "            sample_preds[:, 1:, :, :] = (\n",
    "                sample_preds[:, 1:, :, :] * sample_mask_indicator[:, :, None, None]\n",
    "            )\n",
    "\n",
    "        return sample_images, sample_masks, sample_preds\n",
    "\n",
    "    def _log_images(self, images, batch_mask, batch_pred, pl_module, title=None):\n",
    "        batch_pred = _squash_predictions(batch_pred)  # Shape: (N, H, W)\n",
    "\n",
    "        class_labels = dict(zip(range(1, pl_module._n_classes), miccai.STRUCTURES))\n",
    "        class_labels[0] = \"Void\"\n",
    "\n",
    "        vis_list = []\n",
    "        for i, sample in enumerate(images):\n",
    "            wandb_obj = Image(\n",
    "                sample.permute(1, 2, 0).detach().cpu().numpy(),\n",
    "                masks={\n",
    "                    \"predictions\": {\n",
    "                        \"mask_data\": batch_pred[i].detach().cpu().numpy(),\n",
    "                        \"class_labels\": class_labels,\n",
    "                    },\n",
    "                    \"ground_truth\": {\n",
    "                        \"mask_data\": batch_mask[i].detach().cpu().numpy(),\n",
    "                        \"class_labels\": class_labels,\n",
    "                    },\n",
    "                },\n",
    "            )\n",
    "            vis_list.append(wandb_obj)\n",
    "\n",
    "        pl_module.logger.experiment.log(\n",
    "            {f\"{title}\": vis_list}, step=pl_module.trainer.global_step\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _squash_masks(masks, n_classes, device):\n",
    "    _temp = torch.arange(1, n_classes, device=device)\n",
    "    masks = (masks * _temp[None, :, None, None]).max(dim=1).values\n",
    "    return masks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import LearningRateMonitor,ModelCheckpoint\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning import seed_everything\n",
    "SEED = 12342\n",
    "seed_everything(SEED)\n",
    "dict_args = {\n",
    "    \"experiment_name\":\"UNet 3D\",\n",
    "    'model':'3dunet', #[3dunet,transuneter]\n",
    "    \"gpus\":[0],\n",
    "    \"transform_degree\":0,\n",
    "    \"use_wandb\": True,\n",
    "    \"batch_size\":1, \n",
    "    \"filters\":[64, 128, 256, 512, 1024],\n",
    "    \"use_res_units\":True,\n",
    "    \"downsample\":False,\n",
    "    \"lr\": 1e-3, \n",
    "    \"loss_fx\":[\"GeneralizedDice\",\"Focal\",\"WeightedCrossEntropy\"],\n",
    "    \"exclude_missing\": False, \n",
    "    \"default_root_dir\":DEFAULT_DATA_STORAGE,\n",
    "    \"logger\":WandbLoggerPatch(\n",
    "            name=\"test_3d\",\n",
    "            save_dir=DEFAULT_DATA_STORAGE,\n",
    "            project=\"ct-image-segmentation\",\n",
    "        ),\n",
    "    \"log_every_n_steps\":20\n",
    "}\n",
    "\n",
    "dict_args_trainer = {\n",
    "    \"gpus\":[0],\n",
    "    \"callbacks\": [LearningRateMonitor(logging_interval=\"epoch\"),\\\n",
    "                  EarlyStopping(monitor=\"Mean Dice Score (val)\", patience=50, mode=\"max\"),\\\n",
    "                  ModelCheckpoint(monitor=\"Mean Dice Score (val)\", mode=\"max\")],\n",
    "\n",
    "    \"log_every_n_steps\":20\n",
    "}\n",
    "\n",
    "# Data\n",
    "miccai_3d = MiccaiDataModule3D(**dict_args)\n",
    "\n",
    "# load empty model\n",
    "model = BaseUNet3D(**dict_args)\n",
    "\n",
    "# for transfer model (fix the first half layers)\n",
    "# cnt = 0\n",
    "# fix = 37\n",
    "# for param in model.parameters():\n",
    "#     cnt += 1\n",
    "#     param.requires_grad = False\n",
    "#     if cnt > fix:\n",
    "#         break\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(**dict_args_trainer)\n",
    "trainer.fit(model=model, datamodule=miccai_3d)\n",
    "trainer.test(model=model, datamodule=miccai_3d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = miccai_3d.test_dataset.__getitem__(2)['CT'][tio.DATA].unsqueeze(0)\n",
    "masks = miccai_3d.test_dataset.__getitem__(2)['Segmentation'][tio.DATA].data.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 96, 128, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No contour levels were found within the data range.\n",
      "No contour levels were found within the data range.\n",
      "No contour levels were found within the data range.\n",
      "No contour levels were found within the data range.\n",
      "No contour levels were found within the data range.\n",
      "No contour levels were found within the data range.\n",
      "No contour levels were found within the data range.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(image.shape)\n",
    "prediction = model(image)\n",
    "import matplotlib.pyplot as plt\n",
    "axslice= 63\n",
    "img1=plt.imshow(np.rot90(image[0,0,axslice,:,:],k=3),cmap='gray')\n",
    "img2=plt.contour(np.rot90(masks[0,0,axslice,:,:],k=3),colors='red',linewidths=0.5,linestyles='dotted')\n",
    "img3=plt.contour(np.rot90(masks[0,1,axslice,:,:],k=3),colors='blue',linewidths=0.5,linestyles='dotted')\n",
    "img4=plt.contour(np.rot90(masks[0,2,axslice,:,:],k=3),colors='orange',linewidths=0.5,linestyles='dotted')\n",
    "img5=plt.contour(np.rot90(masks[0,3,axslice,:,:],k=3),colors='green',linewidths=0.5,linestyles='dotted')\n",
    "img6=plt.contour(np.rot90(masks[0,4,axslice,:,:],k=3),colors='yellow',linewidths=0.5,linestyles='dotted')\n",
    "img7=plt.contour(np.rot90(masks[0,5,axslice,:,:],k=3),colors='magenta',linewidths=0.5,linestyles='dotted')\n",
    "img8=plt.contour(np.rot90(masks[0,6,axslice,:,:],k=3),colors='brown',linewidths=0.5,linestyles='dotted')\n",
    "img9=plt.contour(np.rot90(masks[0,7,axslice,:,:],k=3),colors='purple',linewidths=0.5,linestyles='dotted')\n",
    "img10=plt.contour(np.rot90(masks[0,8,axslice,:,:],k=3),colors='pink',linewidths=0.5,linestyles='dotted')\n",
    "plt.title('test ground truth slice = {}'.format(axslice))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No contour levels were found within the data range.\n",
      "No contour levels were found within the data range.\n",
      "No contour levels were found within the data range.\n",
      "No contour levels were found within the data range.\n",
      "No contour levels were found within the data range.\n",
      "No contour levels were found within the data range.\n",
      "No contour levels were found within the data range.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred = _squash_predictions(prediction).detach().numpy()\n",
    "img1=plt.imshow(np.rot90(image[0,0,axslice,:,:],k=3),cmap='gray')\n",
    "img2=plt.contour(np.rot90(np.where(pred==1, 1,0)[0,axslice,:,:],k=3),colors='red',linewidths=0.5,linestyles='dotted')\n",
    "img3=plt.contour(np.rot90(np.where(pred==2, 1,0)[0,axslice,:,:],k=3),colors='blue',linewidths=0.5,linestyles='dotted')\n",
    "img4=plt.contour(np.rot90(np.where(pred==3, 1,0)[0,axslice,:,:],k=3),colors='orange',linewidths=0.5,linestyles='dotted')\n",
    "img5=plt.contour(np.rot90(np.where(pred==4, 1,0)[0,axslice,:,:],k=3),colors='green',linewidths=0.5,linestyles='dotted')\n",
    "img6=plt.contour(np.rot90(np.where(pred==5, 1,0)[0,axslice,:,:],k=3),colors='yellow',linewidths=0.5,linestyles='dotted')\n",
    "img7=plt.contour(np.rot90(np.where(pred==6, 1,0)[0,axslice,:,:],k=3),colors='magenta',linewidths=0.5,linestyles='dotted')\n",
    "img8=plt.contour(np.rot90(np.where(pred==7, 1,0)[0,axslice,:,:],k=3),colors='brown',linewidths=0.5,linestyles='dotted')\n",
    "img9=plt.contour(np.rot90(np.where(pred==8, 1,0)[0,axslice,:,:],k=3),colors='purple',linewidths=0.5,linestyles='dotted')\n",
    "img10=plt.contour(np.rot90(np.where(pred==9, 1,0)[0,axslice,:,:],k=3),colors='pink',linewidths=0.5,linestyles='dotted')\n",
    "plt.title('transfer prediction slice = {}'.format(axslice))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
